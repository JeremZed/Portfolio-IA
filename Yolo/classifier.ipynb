{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7644fc40-3a8a-4771-9807-7fc86a5c2be8",
   "metadata": {},
   "source": [
    "# Classification d'images avec YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7642c5-7a40-4b06-9213-025db1f06216",
   "metadata": {},
   "source": [
    "Utilisation du modÃ¨le YOLO pour classifier les lÃ©gumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb19ccb2-26f9-47fb-a814-17be46879226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"./resources/yolov8n-cls.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# # Train the model\n",
    "# results = model.train(data=\"./resources/dataset\", epochs=10, imgsz=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3c12cf-791c-47ae-b9b4-930275235208",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"./runs/classify/train/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10bd7f7b-7ff3-4bff-84bc-5e51161dff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.34 ðŸš€ Python-3.12.3 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2060, 5919MiB)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1454095 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/jeremy/Lab/repo/Portfolio-IA/Yolo/resources/dataset/train... found 12600 images in 15 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/jeremy/Lab/repo/Portfolio-IA/Yolo/resources/dataset/val... found 3150 images in 15 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /home/jeremy/Lab/repo/Portfolio-IA/Yolo/resources/dataset/test... found 5250 images in 15 classes âœ… \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jeremy/Lab/repo/Portfolio-IA/Yolo/resources/dataset/val... 3150 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3150/3150 [00:00<?, ?it/s]\u001b[0m\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:00<00:00, 215.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.998          1\n",
      "Speed: 0.0ms preprocess, 0.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/val4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9996825456619263"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.top1  # top1 accuracy\n",
    "metrics.top5  # top5 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89165e8e-5fda-4edc-8ca9-5b8def23f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving runs/classify/predict/0035/stage0_Conv_features.png... (16/16)\n",
      "Saving runs/classify/predict/0035/stage1_Conv_features.png... (32/32)\n",
      "Saving runs/classify/predict/0035/stage2_C2f_features.png... (32/32)\n",
      "Saving runs/classify/predict/0035/stage3_Conv_features.png... (32/64)\n",
      "Saving runs/classify/predict/0035/stage4_C2f_features.png... (32/64)\n",
      "Saving runs/classify/predict/0035/stage5_Conv_features.png... (32/128)\n",
      "Saving runs/classify/predict/0035/stage6_C2f_features.png... (32/128)\n",
      "Saving runs/classify/predict/0035/stage7_Conv_features.png... (32/256)\n",
      "Saving runs/classify/predict/0035/stage8_C2f_features.png... (32/256)\n",
      "image 1/1 /home/jeremy/Lab/repo/Portfolio-IA/Yolo/resources/dataset/test/Radish/0035.jpg: 128x128 Radish 1.00, Carrot 0.00, Cucumber 0.00, Potato 0.00, Brinjal 0.00, 4898.8ms\n",
      "Speed: 1.8ms preprocess, 4898.8ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 128)\n",
      "ultralytics.engine.results.Probs object with attributes:\n",
      "\n",
      "data: tensor([1.6722e-06, 1.5899e-06, 4.9744e-06, 8.9890e-06, 4.5083e-06, 1.9375e-06, 1.4305e-06, 3.4820e-05, 4.7786e-06, 1.5358e-05, 1.9419e-06, 1.1027e-05, 6.1875e-06, 9.9989e-01, 8.8846e-06], device='cuda:0')\n",
      "orig_shape: None\n",
      "shape: torch.Size([15])\n",
      "top1: 13\n",
      "top1conf: tensor(0.9999, device='cuda:0')\n",
      "top5: [13, 7, 9, 11, 3]\n",
      "top5conf: tensor([9.9989e-01, 3.4820e-05, 1.5358e-05, 1.1027e-05, 8.9890e-06], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "results = model('./resources/dataset/test/Radish/0035.jpg', visualize=True)\n",
    "for r in results:\n",
    "    print(r.probs)  # print the Probs object containing the detected class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d60756e8-8336-4a32-bb3d-4a18a484ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "for i, r in enumerate(results):\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "\n",
    "    # Show results to screen (in supported environments)\n",
    "    r.show()\n",
    "\n",
    "    # Save results to disk\n",
    "    #r.save(filename=f\"results{i}.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
