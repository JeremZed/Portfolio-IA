{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9698cc53-5930-43ac-9e48-0fca4ef82b70",
   "metadata": {},
   "source": [
    "# Installation de Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c874fd-bdd6-441f-a20a-af8d3c178abf",
   "metadata": {},
   "source": [
    "Dans un terminal linux lancer la commande suivante : \n",
    "\n",
    "```sh\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787878bc-d0ec-4b39-9912-da831fbe6632",
   "metadata": {},
   "source": [
    "### Utilisation d'un modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec2a27-12d5-47b9-a169-c87655f3bb6f",
   "metadata": {},
   "source": [
    "Utilisation du modèle llama3.\n",
    "\n",
    "Si celui-ci n'est pas encore présent en local, alors ollama le télécharge au préalable avant de s'éxécuter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c087552-1d87-4161-bd84-40ef1687077b",
   "metadata": {},
   "source": [
    "```sh\n",
    "ollama run llama3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3c507c-76d1-4a53-a0d1-0195bdeed0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from ollama import Client\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "# print(ollama.invoke(\"quelle était ma première question ?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ca3306-9d2f-41bd-8579-ba898c5e9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphPrompt(input: str, metadata={}, model=\"llama3:latest\"):\n",
    "        \n",
    "    chunk_id = metadata.get('chunk_id', uuid.uuid4().hex)\n",
    "\n",
    "    # model_info = client.show(model_name=model)\n",
    "    # print( chalk.blue(model_info))\n",
    "\n",
    "    SYS_PROMPT = (\"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
    "        \"You are provided with a context chunk (delimited by ```) Your task is to extract the ontology \"\n",
    "        \"of terms mentioned in the given context. These terms should represent the key concepts as per the context. \\n\"\n",
    "        \"Thought 1: While traversing through each sentence, Think about the key terms mentioned in it.\\n\"\n",
    "            \"\\tTerms may include person (agent), location, organization, date, duration, \\n\"\n",
    "            \"\\tcondition, concept, object, entity  etc.\\n\"\n",
    "            \"\\tTerms should be as atomistic\\n\\n\"\n",
    "        \"Thought 2: Think about how these terms can have one on one relation with other terms.\\n\"\n",
    "            \"\\tTerms that are mentioned in the same sentence or the same paragraph are typically related to each other.\\n\"\n",
    "            \"\\tTerms can be related to many other terms\\n\\n\"\n",
    "        \"Thought 3: Find out the relation between each such related pair of terms. \\n\\n\"\n",
    "        \"Format your output as a list of json. Each element of the list contains a pair of terms.\"\n",
    "        \"Never add a comment\"\n",
    "        \"and the relation between them like the follwing. NEVER change the value of the chunk_ID as defined in this prompt: \\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        #'       \"chunk_id\": \"CHUNK_ID_GOES_HERE\",\\n'\n",
    "        '       \"node_1\": \"A concept from extracted ontology\",\\n'\n",
    "        '       \"node_2\": \"A related concept from extracted ontology\",\\n'\n",
    "        '       \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or four words max\"\\n' \n",
    "        \"   }, {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "    # SYS_PROMPT = (\n",
    "    #     \"tu vas avoir pour tâche de faire une analyse onthologique du texte dans le but de générer un graphe de connaissance.\"        \n",
    "    # )\n",
    "    SYS_PROMPT = SYS_PROMPT.replace('CHUNK_ID_GOES_HERE', chunk_id)\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input}``` \\n\\n output: \"\n",
    "\n",
    "    response = client.generate(model, system=SYS_PROMPT, prompt=USER_PROMPT)\n",
    "    \n",
    "    aux1 = response['response']\n",
    "    print(aux1)\n",
    "    # Find the index of the first open bracket '['\n",
    "    start_index = aux1.find('[')\n",
    "    end_index = aux1.find(']')\n",
    "    # Slice the string from start_index to extract the JSON part and fix an unexpected problem with insertes escapes (WHY ?)\n",
    "    json_string = aux1[start_index:end_index+1]\n",
    "    # json_string = json_string.replace('\\\\\\\\\\_', '_')\n",
    "    # json_string = json_string.replace('\\\\\\\\_', '_')\n",
    "    # json_string = json_string.replace('\\\\\\_', '_')\n",
    "    # json_string = json_string.replace('\\\\_', '_')\n",
    "    # json_string = json_string.replace('\\_', '_')\n",
    "    json_string = json_string.replace('```', '')\n",
    "    json_string = json_string.replace(\"\\n\", '')\n",
    "    json_string = json_string.replace(\"\\t\", '')\n",
    "    json_string = json_string.replace(\"  \", '')\n",
    "    json_string.lstrip() # eliminate eventual leading blank spaces\n",
    "    #####################################################\n",
    "    # print(\"json-string:\\n\" + json_string)\n",
    "    #####################################################         \n",
    "    # try:\n",
    "    #     result = json.loads(json_string)\n",
    "    #     result = [dict(item) for item in result]\n",
    "    # except Exception as error:\n",
    "    #     print(\"error ici : \", error)\n",
    "    #     print(\"\\n\\nERROR ### Here is the buggy response: \", response, \"\\n\\n\")\n",
    "    #     result = None\n",
    "    # # print(\"§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§\")\n",
    "\n",
    "    # return result\n",
    "    return aux1\n",
    "\n",
    "def resume(input: str, metadata={}, model=\"llama3:latest\"):\n",
    "    SYS_PROMPT = (\n",
    "        \"tu dois fusionner le contexte afin d'en créer un arbre de connaissance.\"\n",
    "        \"Toutes les entités que tu trouves sont des noeuds\"\n",
    "        \"Une entité peut réprésenter un fruit, un arbre, un lieu, une couleur, une condition, une possibilité, un fait\"\n",
    "        \"Chaque noeud et lien sont représentés par des termes atomiques de 3 mots max\"\n",
    "        \"Le texte est délimité par les caractères ```\"\n",
    "        \"La réponse dois être en français.\"\n",
    "        \"Le format de sortie est un json comme défini dans ce prompt :\"        \n",
    "        '[{ \"node_1\" : \"...\", \"node_2\" : \"...\", \"edge\" : \"lien qui lie le node_1 et le node_2\" }, {...}]'\n",
    "        \"Un noeud doit avoir absolument avec un lien caractérisé par l'attribut 'edge' dans la sortie json\"\n",
    "        \"avant de rendre ta solution tu vérifies l'orthographe des termeset tu les traduis en français\"\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input}``` \\n\\n output: \"\n",
    "\n",
    "    response = client.generate(model, system=SYS_PROMPT, prompt=USER_PROMPT)\n",
    "    \n",
    "    return response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56571830-5ea5-480f-89d5-59bc37e03516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "Here is the extracted ontology and relations:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"arbre\",\n",
      "        \"edge\": \"grows\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pommier\",\n",
      "        \"node_2\": \"arbre\",\n",
      "        \"edge\": \"is type of\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"verte\",\n",
      "        \"edge\": \"has color\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pommier\",\n",
      "        \"node_2\": \"pomme\",\n",
      "        \"edge\": \"produces\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"rouge\",\n",
      "        \"edge\": \"can be in color\"\n",
      "    }\n",
      "]\n",
      "\n",
      "Let me know if you'd like me to explain the reasoning behind each extracted term and relation!\n",
      "test 1\n",
      "[ \n",
      "    { \"node_1\": \"pomme\", \"node_2\": \"arbre\", \"edge\": \"grows in\" }, \n",
      "    { \"node_1\": \"pommier\", \"node_2\": \"arbre\", \"edge\": \"is a type of\" }, \n",
      "    { \"node_1\": \"pomme\", \"node_2\": \"verte\", \"edge\": \"has color\" }, \n",
      "    { \"node_1\": \"arbre\", \"node_2\": \"pommier\", \"edge\": \"hosts\" }, \n",
      "    { \"node_1\": \"pomme\", \"node_2\": \"rouge\", \"edge\": \"can be colored with\" } \n",
      "]\n",
      "test 2\n",
      "Here is the output:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"arbre\",\n",
      "        \"edge\": \"is grown on\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pommier\",\n",
      "        \"node_2\": \"arbre\",\n",
      "        \"edge\": \"is a type of\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"verte\",\n",
      "        \"edge\": \"has the color\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"rouge\",\n",
      "        \"edge\": \"can have the color\"\n",
      "    }\n",
      "]\n",
      "RESUMER :  Here is the output:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"arbre\",\n",
      "        \"edge\": \"is grown on\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pommier\",\n",
      "        \"node_2\": \"arbre\",\n",
      "        \"edge\": \"is a type of\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"verte\",\n",
      "        \"edge\": \"has the color\"\n",
      "    },\n",
      "    {\n",
      "        \"node_1\": \"pomme\",\n",
      "        \"node_2\": \"rouge\",\n",
      "        \"edge\": \"can have the color\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "Let me know if you'd like me to explain the reasoning behind each extracted term and relation!\n"
     ]
    }
   ],
   "source": [
    "sall = []\n",
    "for i in range(3):\n",
    "    print('test', i)\n",
    "    res = graphPrompt(\"La pomme est verte. Elle pousse dans un arbre. Cet arbre s'appelle un pommier. Sachant que sa couleur est verte, est-il possible d'avoir une pomme rouge ?\")\n",
    "    sall.append(res)\n",
    "\n",
    "print(\"RESUMER : \", resume(\"\\n\".join(sall)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
